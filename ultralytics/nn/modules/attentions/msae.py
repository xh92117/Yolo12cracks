# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# å¯¼å…¥é…ç½®
from ultralytics.nn.modules.attentions.config import (
    enable_msae, 
    enable_adaptive_head, 
    msae_config,
    adaptive_head_config
)


class ScaleAwareAttention(nn.Module):
    """
    å°ºåº¦æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—ï¼Œä¸“ä¸ºå¤„ç†ç›®æ ‡å°ºå¯¸å·®å¼‚å¤§çš„æƒ…å†µè®¾è®¡ã€‚
    
    è¯¥æ¨¡å—é€šè¿‡å­¦ä¹ ä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾æƒé‡ï¼ŒåŠ¨æ€è°ƒæ•´å¯¹ä¸åŒå¤§å°ç›®æ ‡çš„å“åº”ï¼Œç‰¹åˆ«é€‚åˆè£‚ç¼æ£€æµ‹åœºæ™¯ã€‚
    
    å‚æ•°:
        in_channels (int): è¾“å…¥é€šé“æ•°
        reduction (int): é€šé“é™ç»´æ¯”ä¾‹
        scale_levels (int): å°ºåº¦åˆ’åˆ†çº§åˆ«
    """
    
    def __init__(self, in_channels, reduction=16, scale_levels=None):
        super().__init__()
        self.in_channels = in_channels
        # ä½¿ç”¨é…ç½®ä¸­çš„å°ºåº¦çº§åˆ«æ•°ï¼Œå¦‚æœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.scale_levels = scale_levels or msae_config.get("scale_levels", 4)
        
        # é€šé“æ³¨æ„åŠ›åˆ†æ”¯
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        # å…±äº«MLPï¼Œä½†æœ‰å¤šç»„æƒé‡ç”¨äºä¸åŒå°ºåº¦
        self.channel_mlp = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
            ) for _ in range(self.scale_levels)
        ])
        
        # é€‰æ‹©æƒé‡
        self.scale_attention = nn.Sequential(
            nn.Conv2d(in_channels * 2, self.scale_levels, 1, bias=False),
            nn.Sigmoid()
        )
        
        # ç©ºé—´æ³¨æ„åŠ›åˆ†æ”¯
        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)
        
        # å°ºåº¦è‡ªé€‚åº”æ¿€æ´»
        self.scale_act = nn.Sigmoid()
    
    def forward(self, x):
        batch_size, _, h, w = x.shape
        
        if not enable_msae:
            # å¦‚æœç¦ç”¨MSAEï¼Œç›´æ¥è¿”å›è¾“å…¥
            return x
            
        # è®¡ç®—å…¨å±€ç‰¹å¾
        avg_out = self.avg_pool(x)
        max_out = self.max_pool(x)
        global_feat = torch.cat([avg_out, max_out], dim=1)
        
        # è®¡ç®—å°ºåº¦æƒé‡
        scale_weights = self.scale_attention(global_feat)  # B x scale_levels x 1 x 1
        
        # å¤šå°ºåº¦é€šé“æ³¨æ„åŠ›
        channel_attn = torch.zeros_like(avg_out)
        for i in range(self.scale_levels):
            scale_attn = self.channel_mlp[i](avg_out) + self.channel_mlp[i](max_out)
            channel_attn += scale_weights[:, i:i+1] * scale_attn
        
        channel_attn = self.scale_act(channel_attn)
        
        # ç©ºé—´æ³¨æ„åŠ›ï¼Œæ•è·è£‚ç¼çš„ç©ºé—´å½¢æ€
        spatial_avg = torch.mean(x, dim=1, keepdim=True)
        spatial_max, _ = torch.max(x, dim=1, keepdim=True)
        spatial_feat = torch.cat([spatial_avg, spatial_max], dim=1)
        spatial_attn = self.scale_act(self.spatial_conv(spatial_feat))
        
        # ç»“åˆé€šé“å’Œç©ºé—´æ³¨æ„åŠ›
        refined = x * channel_attn * spatial_attn
        
        return refined


class MultiScaleAttentionEnhancement(nn.Module):
    """
    å¤šå°ºåº¦æ³¨æ„åŠ›å¢å¼ºæ¨¡å—(MSAE)ï¼Œç”¨äºå¹³è¡¡ä¸åŒå°ºå¯¸ç›®æ ‡çš„æ£€æµ‹æ€§èƒ½ã€‚
    
    è¯¥æ¨¡å—è‡ªé€‚åº”åœ°å¤„ç†ä»å°åˆ°å¤§å„ç§å°ºå¯¸çš„è£‚ç¼ï¼Œæé«˜å°è£‚ç¼çš„æ£€æµ‹ç²¾åº¦çš„åŒæ—¶ä¿æŒå¯¹å¤§è£‚ç¼çš„æ£€æµ‹èƒ½åŠ›ã€‚
    
    å‚æ•°:
        in_channels (list): ä¸åŒç‰¹å¾å±‚çš„é€šé“æ•°åˆ—è¡¨
        width_mult (float): å®½åº¦ä¹˜æ•°ï¼Œæ§åˆ¶æ¨¡å—å¤æ‚åº¦
    """
    
    def __init__(self, in_channels, width_mult=None):
        super().__init__()
        self.n_layers = len(in_channels)
        
        # ä½¿ç”¨é…ç½®ä¸­çš„å®½åº¦ä¹˜æ•°
        self.width_mult = width_mult or msae_config.get("width_mult", 1.0)
        # æ˜¯å¦å¯ç”¨è·¨å°ºåº¦ä¿¡æ¯äº¤æ¢
        self.enable_cross_scale = msae_config.get("enable_cross_scale", True)
        
        # é’ˆå¯¹P3/P4/P5ç‰¹å¾å±‚çš„å°ºåº¦æ³¨æ„åŠ›å¢å¼º
        self.scale_attentions = nn.ModuleList([
            ScaleAwareAttention(
                in_channels[i], 
                reduction=max(int(16 / self.width_mult), 1),
                scale_levels=msae_config.get("scale_levels", 4)
            ) for i in range(self.n_layers)
        ])
        
        if self.enable_cross_scale:
            # è·¨å±‚äº¤äº’ï¼Œå°å°ºåº¦ç‰¹å¾è¾…åŠ©å¤§å°ºåº¦ç‰¹å¾ï¼Œåä¹‹äº¦ç„¶
            self.down_convs = nn.ModuleList([
                nn.Conv2d(in_channels[i], in_channels[i+1], 
                        kernel_size=3, stride=2, padding=1)
                for i in range(self.n_layers-1)
            ])
            
            self.up_convs = nn.ModuleList([
                nn.Sequential(
                    nn.Upsample(scale_factor=2, mode='nearest'),
                    nn.Conv2d(in_channels[i], in_channels[i-1], 
                            kernel_size=1, stride=1)
                ) for i in range(1, self.n_layers)
            ])
            
            # ç‰¹å¾èåˆ
            self.fusions = nn.ModuleList([
                nn.Conv2d(in_channels[i] * 2, in_channels[i], 
                        kernel_size=1, stride=1)
                for i in range(1, self.n_layers-1)
            ])
            
            # P3å±‚ç‰¹æ®Šå¤„ç†ï¼Œåªæ¥æ”¶æ¥è‡ªP4çš„ç‰¹å¾
            self.fusion_p3 = nn.Conv2d(in_channels[0] * 2, in_channels[0], 
                                    kernel_size=1, stride=1)
            
            # P5å±‚ç‰¹æ®Šå¤„ç†ï¼Œåªæ¥æ”¶æ¥è‡ªP4çš„ç‰¹å¾
            self.fusion_p5 = nn.Conv2d(in_channels[-1] * 2, in_channels[-1], 
                                    kernel_size=1, stride=1)
            
            # åŠ¨æ€æƒé‡è°ƒèŠ‚å™¨ï¼Œæ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´ä¸åŒå°ºåº¦çš„é‡è¦æ€§
            self.scale_weights = nn.ModuleList([
                nn.Sequential(
                    nn.AdaptiveAvgPool2d(1),
                    nn.Conv2d(in_channels[i], 1, kernel_size=1),
                    nn.Sigmoid()
                ) for i in range(self.n_layers)
            ])
    
    def forward(self, features):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            features (list): ç‰¹å¾å›¾åˆ—è¡¨ï¼Œé€šå¸¸å¯¹åº”P3, P4, P5ç‰¹å¾å±‚
            
        è¿”å›:
            list: å¢å¼ºåçš„ç‰¹å¾å›¾åˆ—è¡¨
        """
        # å¦‚æœMSAEè¢«ç¦ç”¨ï¼Œç›´æ¥è¿”å›è¾“å…¥
        if not enable_msae:
            return features
            
        # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
        if isinstance(features, list) and len(features) == 1:
            features = features[0]  # å¤„ç†åµŒå¥—åˆ—è¡¨æƒ…å†µ
            
        assert len(features) == self.n_layers, \
            f"Expected {self.n_layers} feature maps, got {len(features)}"
        
        # å…ˆåº”ç”¨å°ºåº¦æ³¨æ„åŠ›
        enhanced = [attn(feat) for feat, attn in zip(features, self.scale_attentions)]
        
        # å¦‚æœä¸å¯ç”¨è·¨å°ºåº¦ä¿¡æ¯äº¤æ¢ï¼Œç›´æ¥è¿”å›ç»è¿‡æ³¨æ„åŠ›å¢å¼ºçš„ç‰¹å¾
        if not self.enable_cross_scale:
            return enhanced
            
        # ç‰¹å¾è‡ªä¸‹è€Œä¸Šä¼ é€’ï¼ˆå°ç›®æ ‡ç‰¹å¾å¸®åŠ©å¤§ç›®æ ‡ï¼‰
        down_feats = [enhanced[0]]
        for i in range(self.n_layers - 1):
            down_feat = self.down_convs[i](down_feats[-1])
            down_feats.append(down_feat)
        
        # ç‰¹å¾è‡ªä¸Šè€Œä¸‹ä¼ é€’ï¼ˆå¤§ç›®æ ‡ç‰¹å¾å¸®åŠ©å°ç›®æ ‡ï¼‰
        up_feats = [enhanced[-1]]
        for i in range(self.n_layers - 1, 0, -1):
            up_feat = self.up_convs[self.n_layers-1-i](up_feats[-1])
            up_feats.append(up_feat)
        
        up_feats = up_feats[::-1]  # åè½¬é¡ºåºä½¿å…¶ä¸enhancedå¯¹åº”
        
        # ç‰¹å¾èåˆ
        results = []
        
        # P3ç‰¹æ®Šå¤„ç†
        p3_weight = self.scale_weights[0](enhanced[0])
        p3_enhanced = torch.cat([enhanced[0], up_feats[0]], dim=1)
        p3_enhanced = self.fusion_p3(p3_enhanced)
        results.append(enhanced[0] + p3_weight * p3_enhanced)
        
        # ä¸­é—´å±‚èåˆï¼šåŒæ—¶æ¥æ”¶ä¸Šä¸‹å±‚ä¿¡æ¯
        for i in range(1, self.n_layers-1):
            fusion_feat = torch.cat([enhanced[i], 
                                    down_feats[i] + up_feats[i]], dim=1)
            fusion_feat = self.fusions[i-1](fusion_feat)
            weight = self.scale_weights[i](enhanced[i])
            results.append(enhanced[i] + weight * fusion_feat)
        
        # P5ç‰¹æ®Šå¤„ç†
        p5_weight = self.scale_weights[-1](enhanced[-1])
        p5_enhanced = torch.cat([enhanced[-1], down_feats[-1]], dim=1)
        p5_enhanced = self.fusion_p5(p5_enhanced)
        results.append(enhanced[-1] + p5_weight * p5_enhanced)
        
        return results


class AdaptiveScaleHead(nn.Module):
    """
    è‡ªé€‚åº”å°ºåº¦æ£€æµ‹å¤´ï¼Œé’ˆå¯¹è£‚ç¼æ£€æµ‹ä¸­çš„å°ºå¯¸å·®å¼‚é—®é¢˜ç‰¹åˆ«ä¼˜åŒ–
    
    ç‰¹ç‚¹ï¼š
    1. é’ˆå¯¹ä¸åŒå°ºåº¦çš„ç›®æ ‡åŠ¨æ€è°ƒæ•´é¢„æµ‹ç­–ç•¥
    2. å¯¹å°ç›®æ ‡å¢å¼ºç‰¹å¾è¡¨è¾¾
    3. å­¦ä¹ ä¸åŒå°ºåº¦ç›®æ ‡çš„æœ€ä½³é”šæ¡†åˆ†é…ç­–ç•¥
    
    å‚æ•°:
        in_channels (list): è¾“å…¥ç‰¹å¾å±‚é€šé“æ•°åˆ—è¡¨
        nc (int): ç±»åˆ«æ•°é‡
        anchors (list): é”šæ¡†é…ç½®
        stride (list): ç‰¹å¾å›¾ç›¸å¯¹åŸå›¾çš„æ­¥é•¿
    """
    
    def __init__(self, in_channels, nc=1, anchors=None, stride=None):
        super().__init__()
        self.nc = nc  # ç±»åˆ«æ•°
        self.na = len(anchors[0]) // 2 if anchors is not None else 3  # æ¯å±‚é”šæ¡†æ•°
        self.no = nc + 5  # æ¯ä¸ªé”šæ¡†çš„è¾“å‡ºæ•°é‡ (ç±»åˆ«+ç½®ä¿¡åº¦+xywh)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªé€‚åº”å¤´éƒ¨
        self.enable_adaptation = adaptive_head_config.get("enable_scale_adaptation", True)
        
        # ä½¿ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›å¢å¼ºæ¨¡å—
        self.msae = MultiScaleAttentionEnhancement(in_channels)
        
        # è½»é‡çº§ç‰¹å¾æå–å™¨ï¼Œå‡å°‘å‚æ•°é‡
        self.extractors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels[i], in_channels[i], 
                          kernel_size=3, padding=1, groups=in_channels[i]),
                nn.BatchNorm2d(in_channels[i]),
                nn.SiLU(),
                nn.Conv2d(in_channels[i], in_channels[i], kernel_size=1),
            ) for i in range(len(in_channels))
        ])
        
        # é¢„æµ‹å¤´ï¼Œæ¯ä¸ªå°ºåº¦ä¸€ä¸ª
        self.heads = nn.ModuleList([
            nn.Conv2d(in_channels[i], self.no * self.na, kernel_size=1)
            for i in range(len(in_channels))
        ])
        
        # å°ºåº¦è‡ªé€‚åº”æ¨¡å—ï¼Œå­¦ä¹ æœ€ä½³é”šç‚¹åˆ†é…å’Œå°ºåº¦æƒé‡
        if self.enable_adaptation:
            self.scale_adapter = nn.ModuleList([
                nn.Sequential(
                    nn.AdaptiveAvgPool2d(1),
                    nn.Conv2d(in_channels[i], 2, kernel_size=1),
                    nn.Sigmoid()
                ) for i in range(len(in_channels))
            ])
        
        # é”šæ¡†è°ƒæ•´çŸ©é˜µï¼Œå¯¹å°ç›®æ ‡çš„é”šæ¡†è¿›è¡Œç‰¹æ®Šè°ƒæ•´
        small_object_ratio = adaptive_head_config.get("small_object_anchor_ratio", [0.8, 1.2])
        self.register_buffer('anchor_adjust', torch.ones(len(in_channels), 2))
        # P3å±‚é”šæ¡†å®½é«˜è°ƒæ•´ç³»æ•°ï¼Œé’ˆå¯¹ç»†å°è£‚ç¼
        self.anchor_adjust[0, :] = torch.tensor(small_object_ratio)  # å°è£‚ç¼å€¾å‘äºæ›´çª„æ›´é•¿
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x (list): è¾“å…¥ç‰¹å¾å›¾åˆ—è¡¨ï¼Œé€šå¸¸ä¸ºP3, P4, P5
            
        è¿”å›:
            tuple: æ£€æµ‹ç»“æœ
        """
        # å¦‚æœè‡ªé€‚åº”å¤´éƒ¨è¢«ç¦ç”¨ï¼Œåˆ™é€€åŒ–ä¸ºæ™®é€šæ£€æµ‹å¤´
        if not enable_adaptive_head:
            outputs = []
            # ä»…å¯¹å¸¸è§„ç‰¹å¾è¿›è¡Œé¢„æµ‹ï¼Œä¸è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´
            for i, feature in enumerate(x):
                # ç‰¹å¾æå–
                feat = self.extractors[i](feature)
                # é¢„æµ‹
                pred = self.heads[i](feat)
                # é‡å¡‘è¾“å‡ºå½¢çŠ¶
                bs, _, ny, nx = pred.shape
                pred = pred.view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
                outputs.append(pred)
            return outputs
        
        # æ£€æŸ¥xçš„æ ¼å¼ï¼Œå¯èƒ½å·²ç»è¢«å¢å¼º
        if not isinstance(x, list) or (isinstance(x, list) and len(x) != 3):
            # å¯èƒ½æ˜¯ä»MSAEä¼ æ¥çš„å·²ç»å¢å¼ºè¿‡çš„ç‰¹å¾
            features = x
        else:
            # åº”ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›å¢å¼º
            features = self.msae(x)
        
        outputs = []
        for i, feature in enumerate(features):
            # ç‰¹å¾æå–
            feat = self.extractors[i](feature)
            
            # é¢„æµ‹
            pred = self.heads[i](feat)
            
            # é‡å¡‘è¾“å‡ºå½¢çŠ¶
            bs, _, ny, nx = pred.shape
            pred = pred.view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
            
            # å¦‚æœå¯ç”¨è‡ªé€‚åº”è°ƒæ•´
            if self.enable_adaptation:
                # åº”ç”¨å°ºåº¦è‡ªé€‚åº”è°ƒæ•´ç³»æ•°
                scale_factor = self.scale_adapter[i](feature)
                
                # æ ¹æ®å°ºåº¦è°ƒæ•´é¢„æµ‹ç»“æœ
                pred[..., 0:2] = pred[..., 0:2] * scale_factor[..., 0].view(bs, 1, 1, 1, 1)  # ä¸­å¿ƒç‚¹ä½ç½®
                pred[..., 2:4] = pred[..., 2:4] * scale_factor[..., 1].view(bs, 1, 1, 1, 1)  # å®½é«˜
            
            outputs.append(pred)
            
        return outputs 